{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWsXrOQGyiNu"
      },
      "source": [
        "# Patch Prediction Models\n",
        "\n",
        "Click to open in: \\[[GitHub](https://github.com/TissueImageAnalytics/tiatoolbox/blob/develop/examples/05-patch-prediction.ipynb)\\]\\[[Colab](https://colab.research.google.com/github/TissueImageAnalytics/tiatoolbox/blob/develop/examples/05-patch-prediction.ipynb)\\]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rc067beJIG86",
        "tags": [
          "remove-cell"
        ]
      },
      "source": [
        "## About this notebook\n",
        "\n",
        "This jupyter notebook can be run on any computer with a standard browser and no prior installation of any programming language is required. It can run remotely over the Internet, free of charge, thanks to Google Colaboratory. To connect with Colab, click on one of the two blue checkboxes above. Check that \"colab\" appears in the address bar. You can right-click on \"Open in Colab\" and select \"Open in new tab\" if the left click does not work for you. Familiarize yourself with the drop-down menus near the top of the window. You can edit the notebook during the session, for example substituting your own image files for the image files used in this demo. Experiment by changing the parameters of functions. It is not possible for an ordinary user to permanently change this version of the notebook on GitHub or Colab, so you cannot inadvertently mess it up. Use the notebook's File Menu if you wish to save your own (changed) notebook.\n",
        "\n",
        "To run the notebook on any platform, except for Colab, set up your Python environment, as explained in the\n",
        "[README](https://github.com/TIA-Lab/tiatoolbox/blob/master/README.md#install-python-package) file.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLUSqCAMyiNz"
      },
      "source": [
        "### About this demo\n",
        "\n",
        "In this example, we will show how to use TIAToolbox for patch-level prediction using a range of deep learning models. TIAToolbox can be used to make predictions on pre-extracted image patches or on larger image tiles / whole-slide images (WSIs), where image patches are extracted on the fly. WSI patch-level predictions can subsequently be aggregated to obtain a segmentation map. In particular, we will introduce the use of our module\n",
        "`patch_predictor` ([details](https://tia-toolbox.readthedocs.io/en/latest/usage.html?highlight=patch_predictor#module-tiatoolbox.models.engine.patch_predictor)). A full list of the available models trained and provided in TIAToolbox for patch-level prediction is given below.\n",
        "\n",
        "- Models trained on the Kather 100k dataset ([details](https://zenodo.org/record/1214456#.YJw4UEhKjvU)):\n",
        "  - `alexnet-kather100k`\n",
        "  - `resnet18-kather100k`\n",
        "  - `resnet34-kather100k`\n",
        "  - `resnet50-kather100k`\n",
        "  - `resnet101-kather100k`\n",
        "  - `resnext50_32x4d-kather100k`\n",
        "  - `resnext101_32x8d-kather100k`\n",
        "  - `wide_resnet50_2-kather100k`\n",
        "  - `wide_resnet101_2-kather100k`\n",
        "  - `densenet121-kather100k`\n",
        "  - `densenet161-kather100k`\n",
        "  - `densenet169-kather100k`\n",
        "  - `densenet201-kather100k`\n",
        "  - `mobilenet_v2-kather100k`\n",
        "  - `mobilenet_v3_large-kather100k`\n",
        "  - `mobilenet_v3_small-kather100k`\n",
        "  - `googlenet-kather100k`\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPiF6kU5yiN0",
        "tags": [
          "remove-cell"
        ]
      },
      "source": [
        "## Setting up the environment\n",
        "\n",
        "### TIAToolbox and dependencies installation\n",
        "\n",
        "You can skip the following cell if 1) you are not using the Colab plaform or 2) you are using Colab and this is not your first run of the notebook in the current runtime session. If you nevertheless run the cell, you may get an error message, but no harm will be done. On Colab the cell installs `tiatoolbox`, and other prerequisite software. Harmless error messages should be ignored. Outside Colab , the notebook expects `tiatoolbox` to already be installed. (See the instructions in [README](https://github.com/TIA-Lab/tiatoolbox/blob/master/README.md#install-python-package).)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "oCOSzUCUXnfh",
        "tags": [
          "remove-cell"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9862b71f-b6ab-4b27-d1b2-b0f3712e15c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\n",
            "Successfully installed SimpleITK-2.4.1 arrow-1.3.0 asciitree-0.3.3 async-lru-2.0.5 bokeh-3.5.2 dicomweb-client-0.59.3 fasteners-0.19 flask-cors-5.0.1 fqdn-1.5.1 fsspec-2024.12.0 glymur-0.13.8 imagecodecs-2024.12.30 isoduration-20.11.0 jedi-0.19.2 json5-0.10.0 jupyter-client-8.6.3 jupyter-events-0.12.0 jupyter-lsp-2.2.5 jupyter-server-2.15.0 jupyter-server-terminals-0.5.3 jupyterlab-4.3.6 jupyterlab-server-2.27.3 marshmallow-3.26.1 numcodecs-0.15.1 numpy-1.26.4 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 openslide-python-1.4.1 overrides-7.7.0 pydicom-3.0.1 python-json-logger-3.3.0 retrying-1.3.4 rfc3339-validator-0.1.4 rfc3986-validator-0.1.1 tiatoolbox-1.6.0 types-python-dateutil-2.9.0.20241206 universal-pathlib-0.2.6 uri-template-1.3.0 wsidicom-0.25.0 zarr-2.18.4\n",
            "Installation is done.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  Running command git clone --filter=blob:none --quiet https://github.com/TissueImageAnalytics/tiatoolbox.git /tmp/pip-req-build-oga2sohh\n",
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.12.0 which is incompatible.\n",
            "notebook 6.5.7 requires jupyter-client<8,>=5.3.4, but you have jupyter-client 8.6.3 which is incompatible.\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "apt-get -y install libopenjp2-7-dev libopenjp2-tools openslide-tools libpixman-1-dev | tail -n 1\n",
        "pip install git+https://github.com/TissueImageAnalytics/tiatoolbox.git@develop | tail -n 1\n",
        "echo \"Installation is done.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evYgBG14yiN2",
        "tags": [
          "remove-cell"
        ]
      },
      "source": [
        "**IMPORTANT**: If you are running this notebook on Colab, then, after running the above cell for the first time, you need to restart the runtime in order to use the latest version of the packages used by TIAToolbox. To do this, you can click on the \"RESTART RUNTIME\" message that appears at the bottom of the cell, or use the menu *Runtime→Restart runtime*. Subsequently, you can use *Runtime→Run all*, **OR** *Runtime→Run after* **OR** run the cells one by one, as you prefer.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DcVZpCBCdxUr",
        "tags": [
          "remove-cell"
        ]
      },
      "source": [
        "**[essential]** Please install the following package, which is required in this notebook.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Y99CwujEdxUr",
        "tags": [
          "remove-cell"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09e915f5-9186-4cdc-d9e0-e4fee720c114"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /usr/local/lib/python3.11/dist-packages (from seaborn) (1.26.4)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.11/dist-packages (from seaborn) (2.2.2)\n",
            "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /usr/local/lib/python3.11/dist-packages (from seaborn) (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "pip install seaborn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGp2XDMAX1GB"
      },
      "source": [
        "### Importing related libraries\n",
        "\n",
        "We import some standard Python modules, and also the TIAToolbox Python modules for the patch classification task, written by the TIA Centre team.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pandas numpy\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3e6FEWq7q5X",
        "outputId": "8e4f7534-6eac-469a-cb74-84c3e75c955e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.2.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "SNbdWfvnFtG5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "707088c8-5832-4f6f-f039-a2d5ae0a59fe"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-06b19e75c353>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m     ) from _err\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m from pandas._config import (\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0mget_option\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mset_option\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/_config/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;34m\"warn_copy_on_write\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m ]\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdates\u001b[0m  \u001b[0;31m# pyright: ignore[reportUnusedImport]  # noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m from pandas._config.config import (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/_config/config.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m from pandas._typing import (\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/_typing.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGenerator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBitGenerator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomState\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    335\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfft\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mattr\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"dtypes\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m             \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtypes\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mattr\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"random\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/random/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;31m# add these for module-freeze analysis (like PyInstaller)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_common\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_bounded_integers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/random/_pickle.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbit_generator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBitGenerator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmtrand\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomState\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_philox\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPhilox\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_pcg64\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPCG64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPCG64DXSM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_sfc64\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSFC64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mnumpy/random/mtrand.pyx\u001b[0m in \u001b[0;36minit numpy.random.mtrand\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
          ]
        }
      ],
      "source": [
        "\"\"\"Import modules required to run the Jupyter notebook.\"\"\"\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "# Clear logger to use tiatoolbox.logger\n",
        "import logging\n",
        "\n",
        "if logging.getLogger().hasHandlers():\n",
        "    logging.getLogger().handlers.clear()\n",
        "\n",
        "import shutil\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "from zipfile import ZipFile\n",
        "\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from matplotlib import cm\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "from tiatoolbox import logger\n",
        "from tiatoolbox.models.engine.patch_predictor import (\n",
        "    IOPatchPredictorConfig,\n",
        "    PatchPredictor,\n",
        ")\n",
        "from tiatoolbox.utils.misc import download_data, grab_files_from_dir, imread\n",
        "from tiatoolbox.utils.visualization import overlay_prediction_mask\n",
        "from tiatoolbox.wsicore.wsireader import WSIReader\n",
        "\n",
        "mpl.rcParams[\"figure.dpi\"] = 160  # for high resolution figure in notebook\n",
        "mpl.rcParams[\"figure.facecolor\"] = \"white\"  # To make sure text is visible in dark mode"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqTCzhhcyiN4",
        "tags": [
          "remove-cell"
        ]
      },
      "source": [
        "### GPU or CPU runtime\n",
        "\n",
        "Processes in this notebook can be accelerated by using a GPU. Therefore, whether you are running this notebook on your own system or on Colab, you need to check and specify appropriate [device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device) e.g., \"cuda\" or \"cpu\" whether you are using GPU or CPU. In Colab, you need to make sure that the runtime type is set to GPU in the *Runtime→Change runtime type→Hardware accelerator*. If you are *not* using GPU, change `device` to `cpu`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YnTUgFLayiN4",
        "tags": [
          "remove-cell"
        ]
      },
      "outputs": [],
      "source": [
        "device = \"cuda\"  # Choose appropriate device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HH93yimOyiN5",
        "tags": [
          "remove-cell"
        ]
      },
      "source": [
        "### Clean-up before a run\n",
        "\n",
        "To ensure proper clean-up (for example in abnormal termination), all files downloaded or created in this run are saved in a single directory `global_save_dir`, which we set equal to \"./tmp/\". To simplify maintenance, the name of the directory occurs only at this one place, so that it can easily be changed, if desired.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YibjAicoAVS1",
        "tags": [
          "remove-cell"
        ]
      },
      "outputs": [],
      "source": [
        "warnings.filterwarnings(\"ignore\")\n",
        "global_save_dir = Path(\"./tmp/\")\n",
        "\n",
        "\n",
        "def rmdir(dir_path: str | Path) -> None:\n",
        "    \"\"\"Helper function to delete directory.\"\"\"\n",
        "    if Path(dir_path).is_dir():\n",
        "        shutil.rmtree(dir_path)\n",
        "        logger.info(\"Removing directory %s\", dir_path)\n",
        "\n",
        "\n",
        "rmdir(global_save_dir)  # remove  directory if it exists from previous runs\n",
        "global_save_dir.mkdir()\n",
        "logger.info(\"Creating new directory %s\", global_save_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TlgYO3n0FtG6"
      },
      "source": [
        "### Downloading the required files\n",
        "\n",
        "We download, over the internet, image files used for the purpose of this notebook. In particular, we download a sample subset of validation patches that were used when training models on the Kather 100k dataset, a sample image tile and a sample whole-slide image. Downloading is needed once in each Colab session and it should take less than 1 minute.\n",
        "In Colab, if you click the file's icon (see below) in the vertical toolbar on the left-hand side then you can see all the files which the code in this notebook can access. The data will appear here when it is downloaded.\n",
        "\n",
        "![image.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACQAAAAlCAYAAAAqXEs9AAAAwElEQVRYhe3WMQ6DMAyFYa7q1Yfw7Dl3ICusZM0hzJpDMLtTGSoFNy2UVPIvvf3DYsignTXcDXjNQVYOsnKQlYOsDkHjOCoiKgBUl3P+DWhZlkPIVagqaJqmt0EAoDFGnefZXEpJt227HtQyZv4chIjKzKeMiHZU7Uom6OhrWhORHSQiDnKQg/oChRD6AjGzg/4L9PyHiEjXdT1lKaUdVEppA7W8h1qHiNUrfv1ibB0RVa9jgu7IQVYOsnKQVXegB/ZWYoL8lUCBAAAAAElFTkSuQmCC)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l7CzZGFHFtG6",
        "tags": [
          "hide-output"
        ]
      },
      "outputs": [],
      "source": [
        "img_file_name = global_save_dir / \"sample_tile.png\"\n",
        "wsi_file_name = global_save_dir / \"sample_wsi.svs\"\n",
        "patches_file_name = global_save_dir / \"kather100k-validation-sample.zip\"\n",
        "imagenet_samples_name = global_save_dir / \"imagenet_samples.zip\"\n",
        "\n",
        "logger.info(\"Download has started. Please wait...\")\n",
        "\n",
        "# Downloading sample image tile\n",
        "download_data(\n",
        "    \"https://tiatoolbox.dcs.warwick.ac.uk/sample_imgs/CRC-Prim-HE-05_APPLICATION.tif\",\n",
        "    img_file_name,\n",
        ")\n",
        "\n",
        "# Downloading sample whole-slide image\n",
        "download_data(\n",
        "    \"https://tiatoolbox.dcs.warwick.ac.uk/sample_wsis/TCGA-3L-AA1B-01Z-00-DX1.8923A151-A690-40B7-9E5A-FCBEDFC2394F.svs\",\n",
        "    wsi_file_name,\n",
        ")\n",
        "\n",
        "# Download a sample of the validation set used to train the Kather 100K dataset\n",
        "download_data(\n",
        "    \"https://tiatoolbox.dcs.warwick.ac.uk/datasets/kather100k-validation-sample.zip\",\n",
        "    patches_file_name,\n",
        ")\n",
        "# Unzip it!\n",
        "with ZipFile(patches_file_name, \"r\") as zipfile:\n",
        "    zipfile.extractall(path=global_save_dir)\n",
        "\n",
        "# Download some samples of imagenet to test the external models\n",
        "download_data(\n",
        "    \"https://tiatoolbox.dcs.warwick.ac.uk/sample_imgs/imagenet_samples.zip\",\n",
        "    imagenet_samples_name,\n",
        ")\n",
        "# Unzip it!\n",
        "with ZipFile(imagenet_samples_name, \"r\") as zipfile:\n",
        "    zipfile.extractall(path=global_save_dir)\n",
        "\n",
        "logger.info(\"Download is complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdaSTKE8FtG7"
      },
      "source": [
        "## Get predictions for a set of patches\n",
        "\n",
        "Below we use `tiatoolbox` to obtain the model predictions for a set of patches with a pretrained model.\n",
        "\n",
        "We use patches from the validation subset of [Kather 100k](https://zenodo.org/record/1214456#.YJ-tn3mSkuU) dataset. This dataset has already been downloaded in the download section above.\n",
        "We first read the data and convert it to a suitable format. In particular, we create a list of patches and a list of corresponding labels.\n",
        "For example, the first label in `label_list` will indicate the class of the first image patch in `patch_list`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5sF4Q-6Px6IV"
      },
      "outputs": [],
      "source": [
        "# read the patch data and create a list of patches and a list of corresponding labels\n",
        "\n",
        "dataset_path = global_save_dir / \"kather100k-validation-sample\"\n",
        "\n",
        "# set the path to the dataset\n",
        "image_ext = \".tif\"  # file extension of each image\n",
        "\n",
        "# obtain the mapping between the label ID and the class name\n",
        "label_dict = {\n",
        "    \"BACK\": 0,\n",
        "    \"NORM\": 1,\n",
        "    \"DEB\": 2,\n",
        "    \"TUM\": 3,\n",
        "    \"ADI\": 4,\n",
        "    \"MUC\": 5,\n",
        "    \"MUS\": 6,\n",
        "    \"STR\": 7,\n",
        "    \"LYM\": 8,\n",
        "}\n",
        "class_names = list(label_dict.keys())\n",
        "class_labels = list(label_dict.values())\n",
        "\n",
        "# generate a list of patches and generate the label from the filename\n",
        "patch_list = []\n",
        "label_list = []\n",
        "for class_name, label in label_dict.items():\n",
        "    dataset_class_path = dataset_path / class_name\n",
        "    patch_list_single_class = grab_files_from_dir(\n",
        "        dataset_class_path,\n",
        "        file_types=\"*\" + image_ext,\n",
        "    )\n",
        "    patch_list.extend(patch_list_single_class)\n",
        "    label_list.extend([label] * len(patch_list_single_class))\n",
        "\n",
        "\n",
        "# show some dataset statistics\n",
        "plt.bar(class_names, [label_list.count(label) for label in class_labels])\n",
        "plt.xlabel(\"Patch types\")\n",
        "plt.ylabel(\"Number of patches\")\n",
        "\n",
        "# count the number of examples per class\n",
        "for class_name, label in label_dict.items():\n",
        "    logger.info(\n",
        "        \"Class ID: %d -- Class Name: %s -- Number of images: %d\",\n",
        "        label,\n",
        "        class_name,\n",
        "        label_list.count(label),\n",
        "    )\n",
        "\n",
        "\n",
        "# overall dataset statistics\n",
        "logger.info(\"Total number of patches: %d\", (len(patch_list)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8tg66bu48Vh"
      },
      "source": [
        "As you can see for this patch dataset, we have 9 classes/labels with IDs 0-8 and associated class names. describing the dominant tissue type in the patch:\n",
        "\n",
        "- BACK ⟶ Background (empty glass region)\n",
        "- LYM ⟶ Lymphocytes\n",
        "- NORM ⟶ Normal colon mucosa\n",
        "- DEB ⟶ Debris\n",
        "- MUS ⟶ Smooth muscle\n",
        "- STR ⟶ Cancer-associated stroma\n",
        "- ADI ⟶ Adipose\n",
        "- MUC ⟶ Mucus\n",
        "- TUM ⟶ Colorectal adenocarcinoma epithelium\n",
        "\n",
        "It is easy to use this code for your dataset - just ensure that your dataset is arranged like this example (images of different classes are placed into different subfolders), and set the right image extension in the `image_ext` variable.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNpCaGiEUvC_"
      },
      "source": [
        "## Predict patch labels in 2 lines of code\n",
        "\n",
        "Now that we have the list of images, we can use TIAToolbox's `PatchPredictor` to predict their category. First, we instantiate a predictor object and then we call the `predict` method to get the results.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "67mRB_YHVXtg",
        "tags": [
          "hide-output"
        ]
      },
      "outputs": [],
      "source": [
        "predictor = PatchPredictor(pretrained_model=\"resnet18-kather100k\", batch_size=32)\n",
        "output = predictor.predict(imgs=patch_list, mode=\"patch\", device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkgUY45WdcxI"
      },
      "source": [
        "Patch Prediction is Done!\n",
        "\n",
        "The first line creates a CNN-based patch classifier instance based on the arguments and prepares a CNN model (generates the network, downloads pretrained weights, etc.). The CNN model used in this predictor can be defined using the `pretrained_model` argument. A complete list of supported pretrained classification models, that have been trained on the Kather 100K dataset, is reported in the first section of this notebook. `PatchPredictor` also enables you to use your own pretrained models for your specific classification application. In order to do that, you might need to change some input arguments for `PatchPredictor`, as we now explain:\n",
        "\n",
        "- `model`: Use an externally defined PyTorch model for prediction, with weights already loaded. This is useful when you want to use your own pretrained model on your own data. The only constraint is that the input model should follow `tiatoolbox.models.abc.ModelABC` class structure. For more information on this matter, please refer to our [example notebook on advanced model techniques](https://github.com/TissueImageAnalytics/tiatoolbox/blob/develop/examples/07-advanced-modeling.ipynb).\n",
        "- `pretrained_model `: This argument has already been discussed above. With it, you can tell tiatoolbox to use one of its pretrained models for the prediction task. A complete list of pretrained models can be found [here](https://tia-toolbox.readthedocs.io/en/latest/usage.html?highlight=pretrained%20models#tiatoolbox.models.architecture.get_pretrained_model). If both `model` and `pretrained_model` arguments are used, then `pretrained_model` is ignored. In this example, we used `resnet18-kather100K,` which means that the model architecture is an 18 layer ResNet, trained on the Kather100k dataset.\n",
        "- `pretrained_weight`: When using a `pretrained_model`, the corresponding pretrained weights will also be downloaded by default. You can override the default with your own set of weights via the `pretrained_weight` argument.\n",
        "- `batch_size`: Number of images fed into the model each time. Higher values for this parameter require a larger (GPU) memory capacity.\n",
        "\n",
        "The second line in the snippet above calls the `predict` method to apply the CNN on the input patches and get the results. Here are some important `predict` input arguments and their descriptions:\n",
        "\n",
        "- `mode`: Type of input to be processed. Choose from `patch`, `tile` or `wsi` according to your application. In this first example, we predict the tissue type of histology patches, so we use the `patch` option. The use of `tile` and `wsi` options are explained below.\n",
        "- `imgs`: List of inputs. When using `patch` mode, the input must be a list of images OR a list of image file paths, OR a Numpy array corresponding to an image list. However, for the `tile` and `wsi` modes, the `imgs` argument should be a list of paths to the input tiles or WSIs.\n",
        "- `return_probabilities`: set to *__True__* to get per class probabilities alongside predicted labels of input patches. If you wish to merge the predictions to generate prediction maps for `tile` or `wsi` modes, you can set `return_probabilities=True`.\n",
        "\n",
        "In the `patch` prediction mode, the `predict` method returns an output dictionary that contains the `predictions` (predicted labels) and `probabilities` (probability that a certain patch belongs to a certain class).\n",
        "\n",
        "The cell below uses common python tools to visualize the patch classification results in terms of classification accuracy and confusion matrix.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KPwyDMCFdcWw"
      },
      "outputs": [],
      "source": [
        "acc = accuracy_score(label_list, output[\"predictions\"])\n",
        "logger.info(\"Classification accuracy: %f\", acc)\n",
        "\n",
        "# Creating and visualizing the confusion matrix for patch classification results\n",
        "conf = confusion_matrix(label_list, output[\"predictions\"], normalize=\"true\")\n",
        "df_cm = pd.DataFrame(conf, index=class_names, columns=class_names)\n",
        "\n",
        "# show confusion matrix\n",
        "sns.heatmap(df_cm, cmap=\"Blues\", annot=True, fmt=\".0%\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtgKKOEGVews"
      },
      "source": [
        "Try changing the `pretrained_model` argument when making `PatchPredictor` instant and see how it can affect the classification output accuracy.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXeRvtXBFtG7"
      },
      "source": [
        "## Get predictions for patches within an image tile\n",
        "\n",
        "We now demonstrate how to obtain patch-level predictions for a large image tile. It is quite a common practice in computational pathology to divide a large image into several patches (often overlapping) and then aggregate the results to generate a prediction map for different regions of the large image. As we are making a prediction per patch again, there is no need to instantiate a new `PatchPredictor` class. However, we should tune the `predict` input arguments to make them suitable for tile prediction. The `predict` function then automatically extracts patches from the large image tile and predicts the label for each of them. As the `predict` function can accept multiple tiles in the input to be processed and each input tile has potentially many patches, we save results in a file when more than one image is provided. This is done to avoid any problems with limited computer memory. However, if only one image is provided, the results will be returned as in `patch` mode.\n",
        "\n",
        "Now, we try this function on a sample image tile. For this example, we use a tile that was released with the [Kather et al. 2016](https://doi.org/10.1038/srep27988) paper. It has been already downloaded in the Download section of this notebook.\n",
        "\n",
        "Let's take a look.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Me1x4O5nXWNV"
      },
      "outputs": [],
      "source": [
        "# reading and displaying a tile image\n",
        "input_tile = imread(img_file_name)\n",
        "\n",
        "plt.imshow(input_tile)\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "\n",
        "logger.info(\n",
        "    \"Tile size is: (%d, %d, %d)\",\n",
        "    input_tile.shape[0],\n",
        "    input_tile.shape[1],\n",
        "    input_tile.shape[2],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uoTL_e5Wl0Pn"
      },
      "source": [
        "## Patch-level prediction in 2 lines of code for big histology tiles\n",
        "\n",
        "As you can see, the size of the tile image is 5000x5000 pixels. This is quite big and might result in computer memory problems if fed directly into a deep learning model. However, the `predict` method of `PatchPredictor` handles this big tile seamlessly by processing small patches independently. You only need to change the `mode` argument to `tile` and a couple of other arguments, as explained below.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AtpW17xglykK",
        "tags": [
          "hide-output"
        ]
      },
      "outputs": [],
      "source": [
        "rmdir(global_save_dir / \"tile_predictions\")\n",
        "img_file_name = Path(img_file_name)\n",
        "\n",
        "predictor = PatchPredictor(pretrained_model=\"resnet18-kather100k\", batch_size=32)\n",
        "tile_output = predictor.predict(\n",
        "    imgs=[img_file_name],\n",
        "    mode=\"tile\",\n",
        "    merge_predictions=True,\n",
        "    patch_input_shape=[224, 224],\n",
        "    stride_shape=[224, 224],\n",
        "    resolution=1,\n",
        "    units=\"baseline\",\n",
        "    return_probabilities=True,\n",
        "    save_dir=global_save_dir / \"tile_predictions\",\n",
        "    device=device,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MvSaH91pTK8"
      },
      "source": [
        "The new arguments in the input of `predict` method are:\n",
        "\n",
        "- `mode='tile'`: the type of image input. We use `tile` since our input is a large image tile.\n",
        "- `imgs`: in tile mode, the input is *required* to be a list of file paths.\n",
        "- `save_dir`: Output directory when processing multiple tiles. We explained before why this is necessary when we are working with multiple big tiles.\n",
        "- `patch_size`: This parameter sets the size of patches (in [W, H] format) to be extracted from the input files, and for which labels will be predicted.\n",
        "- `stride_size`: The stride (in [W, H] format) to consider when extracting patches from the tile. Using a stride smaller than the patch size results in overlapping between consecutive patches.\n",
        "- `labels` (optional): List of labels with the same size as `imgs` that refers to the label of each input tile (not to be confused with the prediction of each patch).\n",
        "\n",
        "In this example, we input only one tile. Therefore the toolbox does not save the output as files and instead returns a list that contains an output dictionary with the following keys:\n",
        "\n",
        "- `coordinates`: List of coordinates of extracted patches in the following format: `[x_min, y_min, x_max, y_max]`. These coordinates can be used to later extract the same region from the input tile/WSI or regenerate a prediction map based on the `prediction` labels for each patch\n",
        "- `predictions`: List of predicted labels for each of the tile's patches.\n",
        "- `label`: Label of the tile generalized to each patch.\n",
        "\n",
        "Keep in mind that if we had several items in the `imgs` input, then the result would be saved in JSON format to the specified `save_dir` and the returned output will be a list of paths to each of the saved JSON files\n",
        "\n",
        "Now, we visualize some patch-level results.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCxf-TXiar7N"
      },
      "source": [
        "### Visualisation of tile results\n",
        "\n",
        "Below we will show some of the results generated by our patch-level predictor on the input image tile. First, we will show some individual patch predictions and then we will show the merged patch level results on the entire image tile.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eft1TtfgpKOm"
      },
      "outputs": [],
      "source": [
        "# individual patch predictions sampled from the image tile\n",
        "\n",
        "# extract the information from output dictionary\n",
        "coordinates = tile_output[0][\"coordinates\"]\n",
        "predictions = tile_output[0][\"predictions\"]\n",
        "\n",
        "# select 4 random indices (patches)\n",
        "rng = np.random.default_rng()  # Numpy Random Generator\n",
        "random_idx = rng.integers(0, len(predictions), (4,))\n",
        "\n",
        "for i, idx in enumerate(random_idx):\n",
        "    this_coord = coordinates[idx]\n",
        "    this_prediction = predictions[idx]\n",
        "    this_class = class_names[this_prediction]\n",
        "\n",
        "    this_patch = input_tile[\n",
        "        this_coord[1] : this_coord[3],\n",
        "        this_coord[0] : this_coord[2],\n",
        "    ]\n",
        "    plt.subplot(2, 2, i + 1), plt.imshow(this_patch)\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(this_class)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TocLP9Bcr4A4"
      },
      "source": [
        "Here, we show a prediction map where each colour denotes a different predicted category. We overlay the prediction map on the original image. To generate this prediction map, we utilize the `merge_predictions` method from the `PatchPredictor` class which accepts as arguments the path of the original image, `predictor` outputs, `mode` (set to `tile` or `wsi`), `tile_resolution` (at which tiles were originally extracted) and `resolution` (at which the prediction map is generated), and outputs the \"Prediction map\", in which regions have indexed values based on their classes.\n",
        "\n",
        "To visualize the prediction map as an overlay on the input image, we use the `overlay_prediction_mask` function from the `tiatoolbox.utils.visualization` module. It accepts as arguments the original image, the prediction map, the `alpha` parameter which specifies the blending ratio of overlay and original image, and the `label_info` dictionary which contains names and desired colours for different classes. Below we generate an example of an acceptable `label_info` dictionary and show how it can be used with `overlay_patch_prediction`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fAQPefwS1GTe"
      },
      "outputs": [],
      "source": [
        "# visualization of merged image tile patch-level prediction.\n",
        "\n",
        "tile_output[0][\"resolution\"] = 1.0\n",
        "tile_output[0][\"units\"] = \"baseline\"\n",
        "\n",
        "label_color_dict = {}\n",
        "label_color_dict[0] = (\"empty\", (0, 0, 0))\n",
        "colors = cm.get_cmap(\"Set1\").colors\n",
        "for class_name, label in label_dict.items():\n",
        "    label_color_dict[label + 1] = (class_name, 255 * np.array(colors[label]))\n",
        "pred_map = predictor.merge_predictions(\n",
        "    img_file_name,\n",
        "    tile_output[0],\n",
        "    resolution=1,\n",
        "    units=\"baseline\",\n",
        ")\n",
        "overlay = overlay_prediction_mask(\n",
        "    input_tile,\n",
        "    pred_map,\n",
        "    alpha=0.5,\n",
        "    label_info=label_color_dict,\n",
        "    return_ax=True,\n",
        ")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qv4SBdcM6GiT"
      },
      "source": [
        "Note that `overlay_prediction_mask` returns a figure handler, so that `plt.show()` or `plt.savefig()` shows or, respectively, saves the overlay figure generated. Now go back and predict with a different `stride_size` or `pretrained_model` to see what effect this has on the output.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxBdhIE-FtG7"
      },
      "source": [
        "## Get predictions for patches within a WSI\n",
        "\n",
        "We demonstrate how to obtain predictions for all patches within a whole-slide image. As in previous sections, we will use `PatchPredictor` and its `predict` method, but this time we set the `mode` to `'wsi'`. We also introduce `IOPatchPredictorConfig`, a class that specifies the configuration of image reading and prediction writing for the model prediction engine.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Kp1kx7wmOYq"
      },
      "outputs": [],
      "source": [
        "wsi_ioconfig = IOPatchPredictorConfig(\n",
        "    input_resolutions=[{\"units\": \"mpp\", \"resolution\": 0.5}],\n",
        "    patch_input_shape=[224, 224],\n",
        "    stride_shape=[224, 224],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h94mfQXfF3lb"
      },
      "source": [
        "Parameters of `IOPatchPredictorConfig` have self-explanatory names, but let's have look at their definition:\n",
        "\n",
        "- `input_resolutions`: a list specifying the resolution of each input head of model in the form of a dictionary. List elements must be in the same order as target `model.forward()`. Of course, if your model accepts only one input, you just need to put one dictionary specifying `'units'` and `'resolution'`. But it's good to know that TIAToolbox supports a model with more than one input.\n",
        "- `patch_input_shape`: Shape of the largest input in (height, width) format.\n",
        "- `stride_shape`: the size of stride (steps) between two consecutive patches, used in the patch extraction process. If the user sets `stride_shape` equal to `patch_input_shape`, patches will be extracted and processed without any overlap.\n",
        "\n",
        "Now that we have set everything, we try our patch predictor on a WSI. Here, we use a large WSI and therefore the patch extraction and prediction processes may take some time (make sure to set the `device=\"cuda\"` if you have access to Cuda enabled GPU and Pytorch+Cuda).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dlQu5878FtG8",
        "tags": [
          "hide-output"
        ]
      },
      "outputs": [],
      "source": [
        "predictor = PatchPredictor(pretrained_model=\"resnet18-kather100k\", batch_size=64)\n",
        "wsi_output = predictor.predict(\n",
        "    imgs=[wsi_file_name],\n",
        "    masks=None,\n",
        "    mode=\"wsi\",\n",
        "    merge_predictions=False,\n",
        "    ioconfig=wsi_ioconfig,\n",
        "    return_probabilities=True,\n",
        "    save_dir=global_save_dir / \"wsi_predictions\",\n",
        "    device=device,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8_S93fSVaFS"
      },
      "source": [
        "We introduce some new arguments for `predict` method:\n",
        "\n",
        "- `mode`: set to 'wsi' when analysing whole slide images.\n",
        "- `ioconfig`: set the IO configuration information using the `IOPatchPredictorConfig` class.\n",
        "- `resolution` and `unit` (not shown above): These arguments specify the level or micron-per-pixel resolution of the WSI levels from which we plan to extract patches and can be used instead of `ioconfig`. Here we specify the WSI's level as `'baseline'`, which is equivalent to level 0. In general, this is the level of greatest resolution. In this particular case, the image has only one level. More information can be found in the [documentation](https://tia-toolbox.readthedocs.io/en/latest/usage.html?highlight=WSIReader.read_rect#tiatoolbox.wsicore.wsireader.WSIReader.read_rect).\n",
        "- `masks`: A list of paths corresponding to the masks of WSIs in the `imgs` list. These masks specify the regions in the original WSIs from which we want to extract patches. If the mask of a particular WSI is specified as `None`, then the labels for all patches of that WSI (even background regions) would be predicted. This could cause unnecessary computation.\n",
        "- `merge_predictions`: You can set this parameter to `True` if you wish to generate a 2D map of patch classification results. However, for big WSIs you might need a large amount of memory available to do this on the file. An alternative (default) solution is to set `merge_predictions=False`, and then generate the 2D prediction maps using `merge_predictions` function as you will see later on.\n",
        "\n",
        "We see how the prediction model works on our whole-slide images by visualizing the `wsi_output`. We first need to merge patch prediction outputs and then visualize them as an overlay on the original image. As before, the `merge_predictions` method is used to merge the patch predictions. Here we set the parameters `resolution=1.25, units='power'` to generate the prediction map at 1.25x magnification. If you would like to have higher/lower resolution (bigger/smaller) prediction maps, you need to change these parameters accordingly. When the predictions are merged, use the `overlay_patch_prediction` function to overlay the prediction map on the WSI thumbnail, which should be extracted at the same resolution used for prediction merging. Below you can see the result:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WF_vY2B4i1yi"
      },
      "outputs": [],
      "source": [
        "# visualization of whole-slide image patch-level prediction\n",
        "overview_resolution = (\n",
        "    4  # the resolution in which we desire to merge and visualize the patch predictions\n",
        ")\n",
        "\n",
        "# the unit of the `resolution` parameter. Can be \"power\", \"level\", \"mpp\", or \"baseline\"\n",
        "overview_unit = \"mpp\"\n",
        "wsi = WSIReader.open(wsi_file_name)\n",
        "wsi_overview = wsi.slide_thumbnail(resolution=overview_resolution, units=overview_unit)\n",
        "plt.figure(), plt.imshow(wsi_overview)\n",
        "plt.axis(\"off\")\n",
        "\n",
        "pred_map = predictor.merge_predictions(\n",
        "    wsi_file_name,\n",
        "    wsi_output[0],\n",
        "    resolution=overview_resolution,\n",
        "    units=overview_unit,\n",
        ")\n",
        "overlay = overlay_prediction_mask(\n",
        "    wsi_overview,\n",
        "    pred_map,\n",
        "    alpha=0.5,\n",
        "    label_info=label_color_dict,\n",
        "    return_ax=True,\n",
        ")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_1pb6BGGbVu"
      },
      "source": [
        "In this notebook, we show how we can use the `PatchPredictor` class and its `predict` method to predict the label for patches of big tiles and WSIs. We introduce `merge_predictions` and `overlay_prediction_mask` helper functions that merge the patch prediction outputs and visualize the resulting prediction map as an overlay on the input image/WSI.\n",
        "\n",
        "All the processes take place within TIAToolbox and you can easily put the pieces together, following our example code. Just make sure to set inputs and options correctly. We encourage you to further investigate the effect on the prediction output of changing `predict` function parameters. Furthermore, if you want to use your own pretrained model for patch classification in the TIAToolbox framework (even if the model structure is not defined in the TIAToolbox model class), you can follow the instructions in our example notebook on [advanced model techniques](https://github.com/TissueImageAnalytics/tiatoolbox/blob/develop/examples/07-advanced-modeling.ipynb) to gain some insights and guidance.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "celltoolbar": "Edit Metadata",
    "colab": {
      "name": "05-patch-prediction.ipynb",
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}